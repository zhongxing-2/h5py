New features
------------

* Defs parser gives now the possibility to add min and max version for each function (https://github.com/h5py/h5py/commit/cc3e6ef3a1d03605d3cb912288fa2d0d42ed2dbc)

* Aligned h5py to updated HDF5 develop branch (current version 1.11.6 - pre-release of 1.12)
* Add `allow_unknown_filter` option to `group.create_dataset`. This should only
  be used for `write_direct_chunk`, and avoid the need to use the low-level API
  to create a dataset for use with `write_direct_chunk`.
* Creating datasets and attributes from str or bytes objects works more
  consistently.
* Casting data to a specified type on reading can now be done more easily,
  with code like ``data = dset.astype(np.int32)[:]``, removing the need for a
  ``with`` statement.
* Exposing the H5Pset_attr_phase_change function to remove the limitation on
  attributes size.
* The chunk query API provides information about dataset chunks in an HDF5 file: location (file offset and byte size), filter mask (which HDF5 filters have been applied when writing the chunk), and chunk offset (zero-based logical position of the chunk’s first element in each dimension).
* The dataset iter_chunks method is an iterator that enables chunk by chunk iteration for all chunks within the given selection.
* Reading typical numeric data now has less overhead, as it can use a fast code
  path implemented mostly in Cython. Making many small reads from the same
  dataset can be as much as 10 times faster, but there are many factors that
  can affect performance.
* Migrate all Cython code base to Cython3 syntax
	* The only noticeable change is in exception raising from cython which use bytes
	* Massively use local imports everywhere as expected from Python3
* Use the libc cimport. Note that the numpy is left untouched, cleanup needed in numpy.pxd
* All the libhdf5 binding is now nogil-enabled but not used
* Use *emalloc* in the _conv module to gracefully fail when no more memory is available
* Dataset objects now expose NumPy-style "nbytes" attribute that returns the size of the dataset's data in bytes.  This differs from the "size" attribute, which returns the size of the dataset in number of elements.
* Attribute Managers will now work as expected on HDF5 datatypes when they previously failed. Resolves issue https://github.com/h5py/h5py/issues/1476.
* The ``external`` argument of :meth:`Group.create_dataset`, which argument
  specifies any external storage for the dataset, accepts more types
  (:issue:`1260`), as follows:

  * The top-level container may be any iterable, not only a list.
  * The names of external files may be not only ``str`` but also ``bytes`` or
    ``os.PathLike`` objects.
  * The offsets and sizes may be *numpy* integers as well as Python integers.

  See also the deprecation related to the ``external`` argument.
* Support for setting file space strategy at file creation.  Includes option to
  persist empty space tracking between sessions.

* The low-level :meth:`FileID.get_vfd_handle` method now works for any
  file driver that supports it, not only the sec2 driver.
* When creating a dataset, shape, chunks and maxshape can be integers
  for 1d arrays (:issue:`1229`).
* When requiring a dataset, shape can be an integer.
* When creating an attribute, shape can be an integer.
* VirtualSource shape and maxshape can be integers.
* VirtualLayout shape and maxshape can be integers.

* It is now possible to call certain hdf5 function without the GIL
  For this, just append the nogil argument in api-function.txt and rebuild
* Function dealing with datasets are now releasing the GIL. This allows to
  interleave IO, GUI and processing nicely from Python.
* Multi-threaded I/O is possible but no performance gain is to be expected !
* Conversion callback (in H5T) are re-acquiring the GIL as needed.
* All other callbacks are now re-acquiring the GIL as needed.
* Release GIL for ``H5Idec_ref``, which can close files.
* Numpy datetime and timedelta arrays can now be stored and read as HDF5
  opaque data (:issue:`1339`), though other tools will not understand them.
  See :ref:`opaque_dtypes` for more information.
* Efficient handling of scalar assignment of chunked datasets when the number of assigned elements is smaller or equal to the size of one chunk.
* Introduced support for split files.
* This prevents seg-fault at exit where Python code may be called after
  the interpreter is already half way destroyed, usually in threaded environment.
* Added function to allow a VDS axis to be an unlimited length mapping.

Deprecations
------------

* In previous versions, creating a dataset from a list of bytes objects would
  choose a fixed length string datatype to fit the biggest item. It will now
  use a variable length string datatype. To store fixed length strings, use a
  suitable dtype from :func:`h5py.string_dtype`.
* The default mode for opening files is now 'r' (read-only).
  See :ref:`file_open` for other possible modes if you need to write to a file.
* The ``external`` argument of :meth:`Group.create_dataset` no longer accepts
  the following forms (:issue:`1260`):

  * a list containing *name*, [*offset*, [*size*]];
  * a list containing *name1*, *name2*, …; and
  * a list containing tuples such as ``(name,)`` and ``(name, offset)`` that
    lack the offset or size.

  Furthermore, each *name*–*offset*–*size* triplet now must be a tuple rather
  than an arbitrary iterable.  See also the new feature related to the
  ``external`` argument.


* The MPI mode no longer supports mpi4py 1.x.
* Remove the deprecated ``h5py.h5t.available_ftypes`` dictionary.
* Remove deprecated ``Dataset.value`` property.
  Use ``ds[()]`` to read all data from any dataset.
* The deprecated functions ``new_vlen``, ``new_enum``, ``get_vlen`` and
  ``get_enum`` have been removed. See :doc:`special` for the newer APIs.
* Removed deprecated File.fid attribute. Use File.id instead.
* Remove the deprecated ``h5py.highlevel`` module.
  The high-level API is available directly in the ``h5py`` module.
* The third argument of ``h5py._hl.selections.select()`` is now an optional
  high-level :class:`Dataset` object, rather than a ``DatasetID``.
  This is not really a public API - it has to be imported through the private
  ``_hl`` module - but probably some people are using it anyway.
* When making a virtual dataset, a dtype must be specified in
  :class:`VirtuaLayout`. There is no longer a default dtype, as this was
  surprising in some cases.

Exposing HDF5 functions
-----------------------

* H5free_memory
* H5Oget_info1
* H5Oget_info_by_name1
* H5Oget_info_by_idx1
* H5Ovisit1
* H5Ovisit_by_name1
* H5Sencode1
* H5Pset_attr_phase_change - https://portal.hdfgroup.org/display/HDF5/H5P_SET_ATTR_PHASE_CHANGE
* H5Dget_num_chunks
* H5Dget_chunk_info
* H5Dget_chunk_info_by_coord
* H5Tget_create_plist - https://support.hdfgroup.org/HDF5/doc/RM/RM_H5T.html#Datatype-GetCreatePlist
* H5Pset_file_space_strategy & H5Pget_file_space_strategy


* H5Pset_fapl_split

Bug fixes
---------

* Some free calls replaced to H5free_memory calls (https://github.com/h5py/h5py/pull/1335#issuecomment-534189716)

* Generic exception types used in tests' assertRaise (exception types changed in new HDF5 version)
* Fix bug when copy source is HLObject and dest is Group (:issue:`1005`).
* Remove `base.MappingHDF5` from `dims.DimensionManager` class definition to handle as collection only, resolves https://github.com/h5py/h5py/issues/744.
* Fix creating attribute with Empty by converting its dtype to a numpy dtype.
* Fix getting ``dset.maxshape`` on empty/null datasets.
* Fix reading data for region references pointing to an empty selection.
* Improved error messages when requesting chunked storage for an empty dataset.
* <news item>

Building h5py
---------

* <news item>
* Fix some errors for internal functions that were raising "TypeError:
  expected bytes, str found" instead of the correct error.
* Fixed ``DatasetID.get_storage_size()`` to report storage size of zero bytes without raising an exception. (https://github.com/h5py/h5py/issues/1475)
* Avoid launching a subprocess by using ``platform.machine()`` at import time.
  This could trigger a warning in MPI.

* Importing an MPI build of h5py no longer initialises MPI immediately,
  which will hopefully avoid various strange behaviours.
* Removed an equality comparison with an empty array, which will cause problems
  with future versions of numpy.
* Better error message if you try to use the mpio driver and h5py was not built
  with MPI support.
* Fix the storage of non-contiguous arrays, such as numpy slices, as HDF5 vlen data.
  Resolves https://github.com/h5py/h5py/issues/1649
* Use relative path for virtual data sources if the source dataset is in the same file.
* Fix pathologically slow reading/writing in certain conditions with integer
  indexing (:issue:`492`).
* The ``File.swmr_mode`` property is consistent with the file's access mode (via ``id.get_intent()``)
* ``File.mode`` property handles SWMR access modes in addition to plain RDONLY/RDWR modes
* As HDF5 1.10.6 and later support UTF-8 paths on Windows, h5py built against
  HDF5 1.10.6 will use UTF-8 for file names.
* fix segment fault issue when accessing vlen of strings #1336

Building h5py
-------------

* It is now possible to specify separate include and library directories for
  HDF5 via both environment variables and setup.py options. See
  :ref:`custom_install`_ for more details.
* The pkg-config name to use when looking up the HDF5 library can now be
  configured, this can assist with selecting the correct HDF5 library when using
  MPI. See :ref:`custom_install`_ for more details.
* use bare ``char*`` instead of ``array.array`` in h5d.read_direct_chunk since
  ``array.array`` is a private CPython C-API interface

* define ``NPY_NO_DEPRECATED_API`` to quiet a warning

* use ``dtype=object`` in tests with ragged arrays
* The ``setup.py configure`` command was removed. Configuration for the build
  can be specified with environment variables instead. See :ref:`custom_install`
  for details.

* Make the lzf filter build with HDF5 1.10  #1219
- If HDF5 is not loaded, an additional message is displayed to check HDF5 installation
* Explicitly mark several Cython functions as non-binding, preparing for the
  default changing in Cython 3.0
* Rely much more on the C-interface provided by cython to expose Python and numpy (to be continued)
* Removed an old workaround which tried to run Cython in a subprocess if
  cythonize() didn't work. This shouldn't be necessary for any recent version
  of setuptools.

Development
-----------


* This allows profiling and code coverage tools to work also
  on cython code whih is great for developers.

